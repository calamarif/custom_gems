from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType
from pyspark.sql.functions import *

from prophecy.cb.server.base import WorkflowContext
from prophecy.cb.server.base.ComponentBuilderBase import ComponentCode, Diagnostic, SeverityLevelEnum
from prophecy.cb.server.base.DatasetBuilderBase import DatasetSpec, DatasetProperties, Component
from prophecy.cb.ui.uispec import *

@dataclass(frozen=True)
class CreateTableProperties:
    key: str
    value: str


class iceberg(DatasetSpec):
    name: str = "iceberg"
    datasetType: str = "Database"
    docUrl: str = "https://docs.prophecy.io/engineers/iceberg/"

    def optimizeCode(self) -> bool:
        return True

    @dataclass(frozen=True)
    class IcebergProperties(DatasetProperties):
        schema: Optional[StructType] = None
        description: Optional[str] = ""
        catalogType: str = "hive"
        catalogName: str = ""
        schemaName: str = ""
        tableName: str = ""
        path: str = ""
        overwriteCondition: str = ""
        externalFilePath: Optional[str] = ""
        writeMode: Optional[str] = "createOrReplace"
        partitionColumns: Optional[List[str]] = None
        mergeSchema: Optional[bool] = None
        isCatalogEnabled: Optional[bool] = None
        timestampAsOf: Optional[str] = None
        snapshotId: Optional[str] = None
        createTableProperties: Optional[List[CreateTableProperties]] = field(default_factory=list)

    def sourceDialog(self) -> DatasetDialog:
        return DatasetDialog("iceberg") \
            .addSection(
            "LOCATION",
            StackLayout()
                .addElement(Checkbox("Use Catalog").bindProperty("isCatalogEnabled"))
                .addElement(
                    StackLayout()
                    .addElement(
                        Condition()
                            .ifEqual(
                             PropExpr("component.properties.isCatalogEnabled"),
                             BooleanExpr(True),
                        )
                        .then(
                            ColumnsLayout(gap="1rem")
                                .addColumn(TextBox("Catalog Name", placeholder="catalog_name").bindProperty("catalogName"))
                                .addColumn(TextBox("Schema Name (Database Name)", placeholder="schema_name").bindProperty("schemaName"))
                                .addColumn(TextBox("Table Name", placeholder="table_name").bindProperty("tableName"))                        
                        )
                        .otherwise(
                            ColumnsLayout(gap="1rem")
                                .addColumn(TextBox("Schema Name (Database Name)", placeholder="schema_name").bindProperty("schemaName"))
                                .addColumn(TextBox("Table Name", placeholder="table_name").bindProperty("tableName"))     
                        )
                    )
                )
            ) \
            .addSection(
            "PROPERTIES",
            ColumnsLayout(gap=("1rem"), height=("100%"))
                .addColumn(
                ScrollBox().addElement(
                    StackLayout(height=("100%")).addElement(
                        StackItem(grow=(1)).addElement(
                            FieldPicker(height=("100%"))
                                .addField(
                                TextArea("Description", 2, placeholder="Data description...").withCopilotEnabledDescribeDataSource(),
                                "description",
                                True
                            )
                                .addField(TextBox("Read timestamp").bindPlaceholder(""), "timestampAsOf")
                                .addField(TextBox("Read snapshot").bindPlaceholder(""), "snapshotId")
                        )
                    )
                ),
                "auto"
            )
                .addColumn(SchemaTable("").bindProperty("schema"), "5fr")
        ) \
            .addSection(
            "PREVIEW",
            PreviewTable("").bindProperty("schema")
        )

    def targetDialog(self) -> DatasetDialog:
        return DatasetDialog("iceberg") \
            .addSection(
            "LOCATION",
            StackLayout()
                .addElement(SelectBox("Catalog Type").addOption("hive", "hive").addOption("hadoop", "hadoop").addOption("other", "other").bindProperty("catalogType"))
                .addElement(Checkbox("Use Catalog").bindProperty("isCatalogEnabled"))
                .addElement(
                    StackLayout()
                    .addElement(
                        Condition()
                            .ifEqual(
                             PropExpr("component.properties.isCatalogEnabled"),
                             BooleanExpr(True),
                        )
                        .then(
                            ColumnsLayout(gap="1rem")
                                .addColumn(TextBox("Catalog Name", placeholder="catalog_name").bindProperty("catalogName"))
                                .addColumn(TextBox("Schema Name (Database Name)", placeholder="schema_name").bindProperty("schemaName"))
                                .addColumn(TextBox("Table Name", placeholder="table_name").bindProperty("tableName"))                        
                        )
                        .otherwise(
                            ColumnsLayout(gap="1rem")
                                .addColumn(TextBox("Schema Name (Database Name)", placeholder="schema_name").bindProperty("schemaName"))
                                .addColumn(TextBox("Table Name", placeholder="table_name").bindProperty("tableName"))     
                        )
                    )
                )
                .addElement(                
                Condition()
                    .ifNotEqual(
                    PropExpr("component.properties.catalogType"),
                    StringExpr("hadoop"),
                )
                    .then(
                    TextBox(
                        "File location (Optional)", placeholder="gs://bucket-name/landing_zn/"
                    ).bindProperty("externalFilePath")
                )
                )
        ) \
        .addSection(
                    "SCHEMA",
                    SchemaTable("").isReadOnly().withoutInferSchema().bindProperty("schema")
                ) \
            .addSection(
            "PROPERTIES",
            ColumnsLayout(gap=("1rem"), height=("100%"))
                .addColumn(
                ScrollBox().addElement(
                    StackLayout(height=("100%"))
                    .addElement(
                        StackItem(grow=(1)).addElement(
                            FieldPicker(height=("100%"))
                                .addField(
                                TextArea("Description", 2, placeholder="Data description...").withCopilotEnabledDescribeDataSource(),
                                "description",
                                True
                            )
                                .addField(SelectBox("Write Mode")
                                          .addOption("createOrReplace", "createOrReplace")
                                          .addOption("overwrite", "overwrite")
                                          .addOption("overwritePartitions", "overwritePartitions")
                                          .addOption("append", "append"),
                                          "writeMode",
                                          True)
                                .addField(Checkbox("Merge dataframe schema into table schema"), "mergeSchema")
                                .addField(
                                SchemaColumnsDropdown("Partition Columns")
                                    .withMultipleSelection()
                                    .bindSchema("schema")
                                    .showErrorsFor("partitionColumns"),
                                "partitionColumns"
                                )    
                        )
                    )
                    .addElement(                
                        Condition()
                        .ifEqual(
                            PropExpr("component.properties.writeMode"),
                            StringExpr("overwrite"),
                        )
                        .then(
                            TextBox(
                                "Overwrite Condition", placeholder="date > '2023-01-23'"
                            ).bindProperty("overwriteCondition")
                        )
                    )
                ),
                "auto"
            ).addColumn(
                    StackLayout(gap=("1rem"))
                        .addElement(
                            AlertBox(
                                variant="info",
                                _children=[
                                    Markdown(
                                        "Write Modes:\n"
                                        "* **createOrReplace**: Create (New) or Replace (Existing) Iceberg table\n"
                                        "* **append**: To append a dataframe to an Iceberg table\n"
                                        "* **overwritePartitions**: To overwrite partitions dynamically\n"
                                        "* **overwrite**: To explicitly overwrite partitions, use overwrite to supply a filter\n"
                                    )
                                ]
                            )
                        )
                        .addElement(TitleElement("Create Table Properties"))
                        .addElement(
                            KeyValuePairs().setPlaceholder(KeyValuePair("bq_connection", "<project>.<region>.<connection>")).bindProperty("createTableProperties")
                        ), 
                "5fr"
            )
        )

    def validate(self, context: WorkflowContext, component: Component) -> list:
        diagnostics = super(iceberg, self).validate(context, component)
        props = component.properties
        if isBlank(props.schema):
            pass
            # diagnostics .append( Diagnostic("properties.schema", "Schema cannot be empty [Properties]", SeverityLevelEnum.Error))

        if (props.timestampAsOf is not None and props.timestampAsOf != "") and (props.snapshotId is not None and props.snapshotId != ""):
            diagnostics.append(
                Diagnostic("properties.timestampAsOf", "Read timestamp and Read snapshot cannot be used together", SeverityLevelEnum.Error))
        
        if props.isCatalogEnabled and (props.catalogName is None or (len(props.catalogName) == 0)):
            diagnostics.append(
                Diagnostic("properties.catalogName", "Catalog Name cannot be empty", SeverityLevelEnum.Error))

        if len(props.schemaName) == 0:
            diagnostics.append(
                Diagnostic("properties.schemaName", "Schema/Database Name cannot be empty", SeverityLevelEnum.Error))

        if len(props.tableName) == 0:
            diagnostics.append(
                Diagnostic("properties.tableName", "Table Name cannot be empty", SeverityLevelEnum.Error))
        
        if props.writeMode == "overwrite" and (props.overwriteCondition is None or props.overwriteCondition == ""):
            diagnostics.append(
                    Diagnostic("properties.overwriteCondition", "If write mode is set to overwrite then overwrite condition is mandatory else use other write mode", SeverityLevelEnum.Error))

        return diagnostics

    def onChange(self, context: WorkflowContext, oldState: Component, newState: Component) -> Component:
        return newState

    class IcebergFormatCode(ComponentCode):
        def __init__(self, props):
            self.props: iceberg.IcebergProperties = props

        def sourceApply(self, spark: SparkSession) -> DataFrame:
            tableName = f"`{self.props.catalogName}`.`{self.props.schemaName}`.`{self.props.tableName}`" if self.props.isCatalogEnabled else f"`{self.props.schemaName}`.`{self.props.tableName}`"
            
            reader = spark.read.format("iceberg")
            if self.props.snapshotId is not None and self.props.snapshotId != "":
                reader = reader.option("snapshot-id", int(self.props.snapshotId))
            if self.props.timestampAsOf is not None and self.props.timestampAsOf != "":
                reader = reader.option("as-of-timestamp", self.props.timestampAsOf)
            return reader.load(tableName)

        def targetApply(self, spark: SparkSession, in0: DataFrame):
            
            database: SubstituteDisabled = None
            
            if self.props.isCatalogEnabled:
                database = f"{self.props.catalogName}.{self.props.schemaName}"
            else:
                database = f"{self.props.schemaName}"
            
            spark.sql(f"CREATE DATABASE IF NOT EXISTS {database};")

            tableName = f"{self.props.catalogName}.{self.props.schemaName}.{self.props.tableName}" if self.props.isCatalogEnabled else f"{self.props.schemaName}.{self.props.tableName}"
            
            tableExists: SubstituteDisabled = False
            if (spark.sql(f"show tables in {database}").filter(col("tableName") == self.props.tableName).count() > 0):
                tableExists = True
               
            writer = in0.writeTo(tableName).using("iceberg")

            if self.props.partitionColumns is not None and len(self.props.partitionColumns) > 0:
                writer = writer.partitionedBy(*self.props.partitionColumns)

            if self.props.mergeSchema is not None and self.props.mergeSchema:
                writer = writer.option("mergeSchema", "true")
                
            if tableExists and self.props.writeMode == "append":
                writer.append()
            elif tableExists and self.props.writeMode == "overwritePartitions":
                writer.overwritePartitions()
            elif tableExists and self.props.writeMode == "overwrite":
                writer.overwrite(expr(self.props.overwriteCondition))
            else:
                if self.props.createTableProperties:
                    for table_property in self.props.createTableProperties:
                        writer = writer.tableProperty(f"{table_property.key}", f"{table_property.value}")
                        
                if self.props.catalogType != "hadoop" and self.props.externalFilePath is not None and self.props.externalFilePath != "":
                    writer = writer.tableProperty("location", self.props.externalFilePath)
                writer.createOrReplace()                