from pyspark.sql import SparkSession, DataFrame, Row
from prophecy.cb.server.base import WorkflowContext
from prophecy.cb.server.base.ComponentBuilderBase import ComponentCode, Diagnostic, SeverityLevelEnum
from prophecy.cb.server.base.DatasetBuilderBase import DatasetSpec, DatasetProperties, Component
from prophecy.cb.ui.uispec import *
from pyspark.sql.types import *
from io import StringIO
import dataclasses
import csv

class Seed(DatasetSpec):
    name: str = "Seed"
    datasetType: str = "File"
    mode: str = "batch"
    docUrl: str = "https://docs.prophecy.io/engineers/seed/"
    

    def optimizeCode(self) -> bool:
        return True

    @dataclass(frozen=True)
    class SeedProperties(DatasetProperties):
        schema: Optional[StructType] = None
        csvData: str = ""
        rows: List[List[str]] = field(default_factory=list)
        separatorType: Optional[str] = "Comma"
        separator: Optional[str] = ","
        header: Optional[bool] = True
        enforceSchema: Optional[bool] = False
        autoInferFromData: Optional[bool] = False

    def sourceDialog(self) -> DatasetDialog:
        delimiterSelectBox = SelectBox("Select an appropriate delimiter for entered data").addOption("Comma", "Comma").addOption("Tab", "Tab").addOption("Custom", "Custom").bindProperty("separatorType")
        schemaTable = Condition().ifEqual(PropExpr("component.properties.autoInferFromData"), BooleanExpr(False)).then(SchemaTable("").bindProperty("schema")).otherwise(SchemaTable("").bindProperty("schema").withoutInferSchema().isReadOnly(True))
        textBox = StackLayout(height = "100%").addElement(TitleElement("Provide your data here")).addElement(Editor(height = "100%", language = "csv").withSchemaSuggestions().bindProperty("csvData").bindLanguage("csv"))
        propertiesWithDelimiterTextBox = StackItem(grow = 1).addElement(
                            FieldPicker(height = "100%")
                            .addField(delimiterSelectBox,"separatorType",True)
                            .addField(
                                TextBox("Column delimiter")
                                .bindPlaceholder(","),
                                "separator",
                                True
                            )
                            .addField(Checkbox("First row is header"), "header", True)
                            .addField(
                                Checkbox("Enforce specified or inferred schema"),
                                "enforceSchema",
                                True
                            )
                            .addField(
                                Checkbox("Auto Infer schema from the data"),
                                "autoInferFromData",
                                True
                            )
                        )
        propertiesWithoutDelimiterTextBox = StackItem(grow = 1).addElement(
                            FieldPicker(height = "100%")
                            .addField(delimiterSelectBox,"separatorType",True)
                            .addField(Checkbox("First row is header"), "header", True)
                            .addField(
                                Checkbox("Enforce specified or inferred schema"),
                                "enforceSchema",
                                True
                            )
                            .addField(
                                Checkbox("Auto Infer schema from the data"),
                                "autoInferFromData",
                                True
                            )
                        )
        propertiesSection = Condition().ifEqual(PropExpr("component.properties.separatorType"), StringExpr("Custom")).then(propertiesWithDelimiterTextBox).otherwise(propertiesWithoutDelimiterTextBox)
        return (DatasetDialog("Seed")
        .addSection("DATA", textBox)
        .addSection(
            "PROPERTIES",
            ColumnsLayout(gap = "1rem", height = "100%")
                .addColumn(
                ScrollBox()
                    .addElement(
                    StackLayout()
                        .addElement(propertiesSection)
                    ),
                "auto"
                )
                .addColumn(schemaTable, "5fr")
        )
        .addSection("PREVIEW",PreviewTable("").bindProperty("schema")))

    def targetDialog(self) -> DatasetDialog:
        return DatasetDialog("Seed")

    def validate(self, context: WorkflowContext, component: Component[SeedProperties]) -> List[Diagnostic]:
        diagnostics = []
        if component.component.lower() == "target":
            diagnostics.append(Diagnostic(
                "properties.csvData",
                "Seed cannot be used as a target.",
                SeverityLevelEnum.Error
            ))
            return diagnostics
        def getDelimiter():
            if component.properties.separatorType == "Comma":
                return ","
            elif component.properties.separatorType == "Tab":
                return "\t"
            else:
                return component.properties.separator

        delimiter = getDelimiter()

        if not component.properties.schema:
            diagnostics.append(Diagnostic(
                "properties.schema",
                "Schema cannot be empty.",
                SeverityLevelEnum.Error
            ))

        rows = []
        if component.properties.csvData != "":
            rows = string_to_csv(component.properties.csvData, delimiter)

        all_valid_rows = len(rows) == 0 or all(len(row) == len(rows[0]) for row in rows)
        if not all_valid_rows:
            diagnostics.append(Diagnostic(
                "properties.rows",
                "Number of columns in each row should be equal.",
                SeverityLevelEnum.Error
            ))

        enforce_schema = component.properties.enforceSchema
        auto_infer_from_data = component.properties.autoInferFromData
        if enforce_schema and auto_infer_from_data:
            diagnostics.append(Diagnostic(
                "properties.autoInferFromData",
                "Enforce schema and auto infer from data cannot be used together.",
                SeverityLevelEnum.Error
            ))

        if component.properties.header and len(rows) < 1:
            diagnostics.append(Diagnostic(
                "properties.csvData",
                "Please provide the header row.",
                SeverityLevelEnum.Error
            ))

        schema_field_size = len(component.properties.schema.fields) if component.properties.schema else 0
        if enforce_schema and len(rows[0]) != schema_field_size:
            diagnostics.append(Diagnostic(
                "properties.schema",
                f"Incompatible schema provided, given data has {len(rows[0])} columns but provided schema has {schema_field_size} fields",
                SeverityLevelEnum.Error
            ))

        return diagnostics

    def onChange(self, context: WorkflowContext, oldState: Component, newState: Component) -> Component:
        def getDelimiter():
            if newState.properties.separatorType == "Comma":
                return ","
            elif newState.properties.separatorType == "Tab":
                return "\t"
            else:
                return newState.properties.separator

        delimiter = getDelimiter()
        def infer_data_types(data, column_names):
            def is_int(s):
                try:
                    int(s)
                    return True
                except ValueError:
                    return False

            def is_number(s):
                try:
                    float(s)
                    return True
                except ValueError:
                    return False

            data_types = {}
            for index, name in enumerate(column_names):
                column_data = [row[index] for row in data]
                if all(not x or is_int(x) for x in column_data):
                    data_types[name] = IntegerType()
                elif all(not x or is_number(x) for x in column_data):
                    data_types[name] = DoubleType()
                else:
                    data_types[name] = StringType()

            return data_types
        rows = string_to_csv(newState.properties.csvData, delimiter)
        newSchema = newState.properties.schema
        if newState.properties.autoInferFromData:
            column_names = rows[0]
            if newState.properties.header == False:
                column_names = [f"_c{i}" for i in range(len(rows[0]))]
            dataRows = rows[1:]
            if newState.properties.header == False:
                dataRows = rows
            data_types = infer_data_types(dataRows, column_names)
            schema_fields = [StructField(name, dtype, True) for name, dtype in data_types.items()]
            newSchema = StructType(schema_fields)

        newProps = newState.properties
        if newState.properties.autoInferFromData:
            return newState.bindProperties(dataclasses.replace(newProps, rows = rows, schema = newSchema))
        else:
            return newState.bindProperties(dataclasses.replace(newProps, rows = rows))

    class SeedFormatCode(ComponentCode):
        def __init__(self, props):
            self.props: Seed.SeedProperties = props

        def sourceApply(self, spark: SparkSession) -> DataFrame:
            if self.props.schema is not None:
                schemaFields:SubstituteDisabled = self.props.schema.fields
                readSchema:SubstituteDisabled = StructType([StructField(f.name, StringType(), True) for f in schemaFields])
                castExpressions:SubstituteDisabled = [col(f.name).cast(f.dataType) for f in schemaFields]
                if self.props.autoInferFromData and self.props.header:
                    rows = [Row(*r) for r in self.props.rows[1:]]
                    return spark.createDataFrame(rows, readSchema).select(castExpressions)
                elif self.props.autoInferFromData:
                    rows = [Row(*r) for r in self.props.rows]
                    return spark.createDataFrame(rows, readSchema).select(castExpressions)
                elif self.props.header and self.props.enforceSchema:
                    rows = [Row(*r) for r in self.props.rows[1:]]
                    return spark.createDataFrame(rows, readSchema).select(castExpressions)
                elif self.props.header and not self.props.enforceSchema:
                    rows = [Row(*r) for r in self.props.rows[1:]]
                    return spark.createDataFrame(rows, readSchema)
                else:
                    rows = [Row(*r) for r in self.props.rows]
                    return spark.createDataFrame(rows, readSchema).select(castExpressions)
            else:
                if self.props.header:
                    rows = [Row(*r) for r in self.props.rows[1:]]
                    string_schema = StructType([StructField(f, StringType(), True) for f in props.rows[0]])
                    return spark.createDataFrame(rows, string_schema)
                else:
                    num_cols = len(self.props.rows[0])
                    column_names = [f"_c{i}" for i in range(num_cols)]
                    schema = StructType([StructField(name, StringType(), True) for name in column_names])
                    rows = [Row(*r) for r in self.props.rows]
                    return spark.createDataFrame(rows, schema)


        def targetApply(self, spark: SparkSession, in0: DataFrame):
            pass

def string_to_csv(input_string, delimiter=","):
    if not input_string:
        return [[]]

    return [parse_row(row, delimiter) for row in input_string.split('\n')]

def parse_row(row, delimiter):
    fields = []
    builder = []
    in_quotes = False
    i = 0
    delimiter_length = len(delimiter)
    if delimiter_length == 0:
        return [row]

    if len(row) == 0:
        fields.append("")
    else:
        while i < len(row):
            # Check for delimiter only when not inside quotes
            if not in_quotes and row[i:i + delimiter_length] == delimiter:
                fields.append(''.join(builder))
                builder.clear()
                i += delimiter_length - 1  # Adjust to skip past the full delimiter
            else:
                if row[i] == '\\' and i + 1 < len(row) and row[i + 1] == '"':
                    builder.append('"')
                    i += 1  # Skip the escaped quote
                elif row[i] == '"':
                    in_quotes = not in_quotes  # Toggle in_quotes
                else:
                    builder.append(row[i])
            i += 1
        fields.append(''.join(builder))
    return fields
