from prophecy.cb.server.base.MetaComponentBuilderBase import *
from prophecy.cb.server.base.ComponentBuilderBase import *
from prophecy.cb.util.config import ConfigurationRecordField
from prophecy.utils import do_union
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.types import StructField

from prophecy.cb.server.base import WorkflowContext
from prophecy.cb.server.base.datatypes import SInt
from prophecy.cb.ui.uispec import *
from prophecy.config import ConfigBase
from typing import *
from prophecy.cb.util.Utils import generate_element_id
import builtins, dataclasses


@dataclass(frozen=True)
class ConfigToColumnMap:
    configName: str
    columnName: str

class TableIterator(MetaComponentSpec, ColumnConfigAutoPopulateSpec):
    name: str = "TableIterator"
    category: str = "ControlFlow"
    gemDescription: str = "This subgraph iterates over each row of the first input dataframe (table)."
    docUrl: str = "http://docs.prophecy.io/low-code-spark/gems/subgraph/table-iterator"
    interimRowName: str = "Iteration"
    portNumberToProp: list = [(0, "${component.properties.configToColumnMap}")]

    @dataclass(frozen=True)
    class TableIteratorProperties(MetaComponentProperties):
        maxIteration: SInt = SInt("1000")
        parallelismEnabled: Optional[bool] = False
        numberOfThreads: SInt = SInt("5")
        configToColumnMap: List[ConfigToColumnMap] = field(default_factory=list)
        availableConfigFieldNames: List[str] = field(default_factory=list)
        availableColumnNames: List[str] = field(default_factory=list)


    def dialog(self) -> Dialog:
        return (
            Dialog("TableIterator", footer=SubgraphDialogFooter())
            .addElement(
                ColumnsLayout(gap="1rem", height="100%")
                .addColumn(Ports(minInputPorts=1, allowOutputRename=True, allowOutputAddOrDelete=True).editableInput(True), "content")
                .addColumn(
                    Tabs()
                    .addTabPane(
                        TabPane("Iterator Settings", "Settings")
                        .addElement(
                            StackLayout(height="100%", gap="2rem")
                            .addElement(
                                ExpressionBox("Max Iterations")
                                .bindPlaceholder("1000")
                                .bindProperty("maxIteration")
                                .withFrontEndLanguage()
                            )
                            .addElement(Checkbox("Enable Parallel Execution", "parallelismEnabled"))
                            .addElement(
                                Condition()
                                .ifEqual(
                                    PropExpr("component.properties.parallelismEnabled"), BooleanExpr(True)
                                )
                                .then(
                                    ExpressionBox("Number of threads")
                                    .bindPlaceholder("5")
                                    .bindProperty("numberOfThreads")
                                    .withFrontEndLanguage()
                                )
                            )
                            .addElement(
                                ConfigurationSelectorTable("ConfigToColumnMapTable")
                                .bindProperty("configToColumnMap")
                                .bindConfigFieldNames("${component.properties.availableConfigFieldNames}")
                                .bindColumnNames("${component.properties.availableColumnNames}")
                                .bindPortIndex(0)
                            )
                        )
                    )
                    .addTabPane(
                        TabPane("Configuration", "Configuration")
                        .addElement(
                            SubgraphConfigurationTabs()
                        )
                    ),
                    "3fr"
                )
            )
        )

    def validate(self, context: WorkflowContext, component: MetaComponent) -> List[Diagnostic]:
        diagnostics = []
        maxIterationDiagMsg = "MaxIteration has to be an integer > 0"
        if component.properties.maxIteration.diagnosticMessages is not None and len(
                component.properties.maxIteration.diagnosticMessages) > 0:
            for message in component.properties.maxIteration.diagnosticMessages:
                diagnostics.append(Diagnostic("properties.maxIteration", message, SeverityLevelEnum.Error))
        else:
            resolved = component.properties.maxIteration.value
            if resolved < 0:
                diagnostics.append(Diagnostic("properties.maxIteration", maxIterationDiagMsg, SeverityLevelEnum.Error))
        numThreadsDiagMsg = "Number of threads has to be an integer > 0"
        if component.properties.numberOfThreads.diagnosticMessages is not None and len(
                component.properties.numberOfThreads.diagnosticMessages) > 0:
            for message in component.properties.numberOfThreads.diagnosticMessages:
                diagnostics.append(Diagnostic("properties.numberOfThreads", message, SeverityLevelEnum.Error))
        else:
            resolved = component.properties.numberOfThreads.value
            if resolved < 0:
                diagnostics.append(Diagnostic("properties.numberOfThreads", numThreadsDiagMsg, SeverityLevelEnum.Error))
        if len(component.ports.inputs) > 0:
            if component.ports.inputs[0].isStreaming:
                diagnostics.append(
                    Diagnostic(
                        f"ports",
                        "Cannot iterate on a streaming dataframe. Please connect the first input to a batch Source instead of a streaming Source.",
                        SeverityLevelEnum.Error
                    )
                )
        if (len(component.ports.inputs) != len(component.subgraph_ports.inputs) + 1):
            diagnostics.append(
                Diagnostic(
                    f"ports",
                    f"Output ports of TableIterator's input component should be 1 less than input ports. Expected: {len(component.ports.inputs) - 1} ports. Found: {len(component.subgraph_ports.inputs)} ports",
                    SeverityLevelEnum.Error
                )
            )
        if (len(component.ports.outputs) != len(component.subgraph_ports.outputs)):
            diagnostics.append(
                Diagnostic(
                    f"ports",
                    f"Count of input and output ports of TableIterator's Output component should be same. Input port count::{len(component.subgraph_ports.outputs)} Output port count:: {len(component.ports.outputs)}",
                    SeverityLevelEnum.Error
                )
            )
        index = -1
        for conf_to_col in component.properties.configToColumnMap:
            index = index + 1
            config_name, column_name = (conf_to_col.configName, conf_to_col.columnName)
            if config_name is None or len(config_name.strip()) == 0:
                diagnostics.append(
                    Diagnostic(
                        f"properties.configToColumnMap[{index}].configName",
                        f"IteratorSettings error:: Config Name cannot be empty in column to config mapping. Please select a config from the dropdown.",
                        SeverityLevelEnum.Error
                    )
                )
            elif column_name is None or len(column_name.strip()) == 0:
                diagnostics.append(
                    Diagnostic(
                        f"properties.configToColumnMap[{index}].columnName",
                        f"IteratorSettings error:: Column Name cannot be empty in column to config mapping. Please select a column from the dropdown.",
                        SeverityLevelEnum.Error
                    )
                )
            else:
                availableConfigFieldNames = context.config_context.get_field_names()
                availableColumnNames = []
                if (len(component.ports.inputs) > 0):
                    schema = component.ports.inputs[0].schema
                    availableColumnNames = schema.fieldNames() if schema is not None else []
                if config_name not in availableConfigFieldNames:
                    diagnostics.append(
                        Diagnostic(
                            f"properties.configToColumnMap[{index}].configName",
                            f"IteratorSettings error:: Column: {column_name} maps to Config Field with name: {config_name} but config field: {config_name} is unavailable. Config might be broken or field not defined. Please check.",
                            SeverityLevelEnum.Error
                        )
                    )
                elif column_name not in availableColumnNames:
                    diagnostics.append(
                        Diagnostic(
                            f"properties.configToColumnMap[{index}].columnName",
                            f"IteratorSettings error:: Column: {column_name} maps to Config Field with name: {config_name} but column Name with name: {column_name} not found.",
                            SeverityLevelEnum.Error
                        )
                    )
                else:
                    matched_config_field: ConfigurationRecordField = context.config_context.config.get_record().get_config_field_by_name(config_name)
                    config_df_schema = component.ports.inputs[0].schema if len(component.ports.inputs) > 0 else None
                    if matched_config_field is not None and config_df_schema is not None:
                        matching_column: Optional[StructField] = None
                        for field in config_df_schema.fields:
                            if field.name == column_name:
                                matching_column = field
                        errors = matched_config_field.validate_with_spark_schema([], matching_column, False)
                        for (path, error) in errors:
                            diagnostics.append(
                                Diagnostic(
                                    f"properties.configToColumnMap[{index}].configName",
                                    f"IteratorSettings error:: Config and Column type mismatch for config:{config_name} column:{column_name}. Error:: {error}",
                                    SeverityLevelEnum.Error
                                )
                            )
        return diagnostics

    def onChange(self, context: WorkflowContext, oldState: MetaComponent, newState: MetaComponent) -> MetaComponent:
        availableConfigFieldNames = context.config_context.get_field_names()
        availableColumnNames = []
        updatedState = newState

        oldInputPorts = oldState.ports.inputs
        newInputPorts = newState.ports.inputs
        port_index = -1
        subgraph_input_ports = newState.subgraph_ports.inputs
        subgraph_input_ports_updated = []
        subgraph_input_ports_to_remove = []
        subgraph_input_ports_to_add = []
        # Process input port removal
        for oldInputPort in oldInputPorts:
            port_index += 1
            matching_new_node = list(builtins.filter(lambda port: port.id == oldInputPort.id, newInputPorts))
            if (len(matching_new_node) == 0):
                # remove the (port_index - 1) from subgraph_ports.input
                subgraph_input_ports_to_remove.append(port_index)
        if len(subgraph_input_ports) != len(newInputPorts) - 1:
            for index in range(0, len(subgraph_input_ports)):
                if (index in subgraph_input_ports_to_remove) and (index == 0):
                    # First dataframe is config dataframe. There is no matching __run__() method port for this.
                    subgraph_input_ports_updated.append(subgraph_input_ports[index])
                elif ((index+1) not in subgraph_input_ports_to_remove):
                    subgraph_input_ports_updated.append(subgraph_input_ports[index])
        else:
            subgraph_input_ports_updated = subgraph_input_ports
        # Process input node addition
        def available_slugs(prefix, slugs):
            index = 0
            while (index < 1000):
                slug = f"{prefix}{index}"
                if slug not in slugs:
                    return slug
                index += 1
        port_index = -1
        for newInputPort in newInputPorts:
            port_index += 1
            matching_old_node = list(builtins.filter(lambda port: port.id == newInputPort.id, oldInputPorts))
            if (len(matching_old_node) == 0):
                subgraph_input_ports_to_add.append(port_index)
        if len(subgraph_input_ports) != len(newInputPorts) - 1:
            current_slugs = [x.slug for x in subgraph_input_ports]
            for new_port_index in subgraph_input_ports_to_add:
                if new_port_index == 0:
                    # Config dataframe has got added. Do nothing.
                    continue
                elif new_port_index > 0:
                    new_port = NodePort(generate_element_id(), available_slugs("out", current_slugs))
                    subgraph_input_ports_updated = subgraph_input_ports_updated[0:(new_port_index-1)] + [new_port] + subgraph_input_ports_updated[(new_port_index-1):]

        oldOutputPorts = oldState.ports.outputs
        newOutputPorts = newState.ports.outputs

        port_index = -1
        subgraph_output_ports = newState.subgraph_ports.outputs
        subgraph_output_ports_updated = []
        subgraph_output_ports_to_remove = []
        subgraph_output_ports_to_add = []
        for old_port in oldOutputPorts:
            port_index += 1
            matching_new_node = list(builtins.filter(lambda port: port.id == old_port.id, newOutputPorts))
            if (len(matching_new_node) == 0):
                # remove the (port_index) from subgraph_ports.output
                subgraph_output_ports_to_remove.append(port_index)
        if len(subgraph_output_ports) != len(newOutputPorts):
            for index in range(0, len(subgraph_output_ports)):
                if (index not in subgraph_output_ports_to_remove):
                    subgraph_output_ports_updated.append(subgraph_output_ports[index])
        else:
            subgraph_output_ports_updated = subgraph_output_ports
        port_index = -1
        for new_port in newOutputPorts:
            port_index += 1
            matching_old_node = list(builtins.filter(lambda port: port.id == new_port.id, oldOutputPorts))
            if (len(matching_old_node) == 0):
                subgraph_output_ports_to_add.append(port_index)
        if len(subgraph_output_ports) != len(newOutputPorts):
            current_slugs = [x.slug for x in subgraph_output_ports]
            for new_port_index in subgraph_output_ports_to_add:
                new_port = NodePort(generate_element_id(), available_slugs("in", current_slugs))
                subgraph_output_ports_updated = subgraph_output_ports_updated[0:(new_port_index - 1)] + [new_port] + subgraph_output_ports_updated[(new_port_index - 1):]
        new_subgraph_ports = dataclasses.replace(newState.subgraph_ports, inputs=subgraph_input_ports_updated, outputs=subgraph_output_ports_updated)
        updatedState = dataclasses.replace(updatedState, subgraph_ports=new_subgraph_ports)

        isInputNodePortCombCorrect = (len(updatedState.ports.inputs) == len(updatedState.subgraph_ports.inputs) + 1)
        isOutputNodePortCombCorrect = (len(updatedState.ports.outputs) == len(updatedState.subgraph_ports.outputs))
        if (len(updatedState.ports.inputs) > 0):
            schema = updatedState.ports.inputs[0].schema
            availableColumnNames = schema.fieldNames() if schema is not None else []
        if (len(updatedState.ports.inputs) > 1) or (not isInputNodePortCombCorrect) or (len(updatedState.subgraph_ports.inputs) > 0):
            updatedState = dataclasses.replace(updatedState, visualProperty=dataclasses.replace(updatedState.visualProperty, isInputNodeHidden=False))
        else:
            updatedState = dataclasses.replace(updatedState, visualProperty=dataclasses.replace(updatedState.visualProperty, isInputNodeHidden=True))
        if (len(updatedState.ports.outputs) > 0) or (not isOutputNodePortCombCorrect) or (len(updatedState.subgraph_ports.outputs) > 0):
            updatedState = dataclasses.replace(updatedState, visualProperty=dataclasses.replace(updatedState.visualProperty, isOutputNodeHidden=False))
        else:
            updatedState = dataclasses.replace(updatedState, visualProperty=dataclasses.replace(updatedState.visualProperty, isOutputNodeHidden=True))
        updatedState = updatedState.bindProperties(
            dataclasses.replace(updatedState.properties, availableColumnNames=availableColumnNames, availableConfigFieldNames=availableConfigFieldNames)
        )
        return updatedState

    class TableIteratorCode(MetaComponentCode):
        def __init__(self, newProps, config):
            self.props: TableIterator.TableIteratorProperties = newProps
            self.config: ConfigBase = config

        def apply(self, spark: SparkSession, in0: DataFrame, *inDFs: DataFrame) -> (List[DataFrame]):
            results = []
            conf_to_column = dict([(x.configName, x.columnName) for x in self.props.configToColumnMap])
            if self.props.maxIteration is not None and in0.count() > self.props.maxIteration.value:
                raise Exception(f"Config DataFrame row count::{in0.count()} exceeds max run count")
            if self.props.parallelismEnabled:
                import multiprocessing
                from multiprocessing.pool import ThreadPool
                from functools import partial
                num_processes = self.props.numberOfThreads.value
                rows = in0.collect()
                with ThreadPool(processes=num_processes) as pool:
                    def process_row(row, config, inDFs, spark):
                        update_config = config.update_from_row_map(row, conf_to_column)
                        return self.__run__(spark, update_config, *inDFs)

                    partial_process_row: SubstituteDisabled = partial(process_row, config=self.config, inDFs=inDFs, spark=spark)
                    results: SubstituteDisabled = pool.map(partial_process_row, rows)
                    return do_union(results)
            else:
                for row in in0.collect():
                    update_config = self.config.update_from_row_map(row, conf_to_column)
                    _inputs = inDFs
                    results.append(self.__run__(spark, update_config, *_inputs))
                return do_union(results)