import builtins
from typing import List, Dict

from pyspark.sql.types import *

from prophecy.cb.ui.uispec import SColumn


class BusinessRuleOutput:
    def __init__(self, name: str):
        self.name = name


class BusinessRule:

    def __init__(self, name: str, inputs: List[StructField], outputs: List[BusinessRuleOutput], expression: SColumn):
        self.name = name
        self.inputs = inputs
        self.outputs = outputs
        self.expression = expression

    def get_name(self):
        """
        Returns the name of the rule.
        """
        return self.name

    def get_input_columns(self):
        """
        Returns the name of the input column params of the rule.
        """
        return [x.name for x in self.inputs]

    def get_input_column_parts(self, input_col_expr: str):
        """
        This will break the column expression into parts.
        For ex: a.b.c -> ["a", "b", "c"], `a.b`.c.`def gh` -> ["a.b", "c", "def gh"]
        """
        from prophecy.cb.ui.UISpecUtil import getColumnExprParts
        getColumnExprParts(input_col_expr)

    def matches_expression(self, expression: str):
        """
        Checks if the given expression is for this rule. Format: rule_name( )
        """
        # TODO :: Maybe use tokenizer module to tokenize the expression and do a full match
        trimmed_expr = expression.lstrip()
        first_paran_index = trimmed_expr.find("(")
        if first_paran_index != -1:
            func_name = trimmed_expr[:first_paran_index]
            return (func_name == self.name)
        else:
            return False

    def validate_inputs(self, input_port_schema: StructType, target_columns: List[str] = None) -> Dict[str, list]:
        """
        Expected params:
            input_port_schema: StructType -> input port schema of the given component
            target_columns: List[str] -> target_columns as list of string
        This method validates whether the input column parameters of the given rule is there in the input port schema
        with appropriate datatype or not. If target_columns are passed, then it checks whether there exists a target column
        same as that of rule input.
        """
        from prophecy.cb.server.base.ComponentBuilderBase import Diagnostic, SeverityLevelEnum
        from prophecy.cb.ui.UISpecUtil import getColumnExprParts
        from prophecy.cb.util.Utils import get_spark_datatypes_diff
        param_errors = {}
        for rule_input in self.inputs:
            try:
                column_found_in_port_schema = False
                schema_mismatched = False
                schema_mismatch_errors = {}
                found_in_target_col = False
                col_expr_parts = getColumnExprParts(rule_input.name)
                if len(col_expr_parts) > 0:
                    expr_first_part = col_expr_parts[0]
                    matching_input_port_columns: List[StructField] = list(
                        builtins.filter(lambda input_field: input_field.name.lower() == expr_first_part.lower(), input_port_schema.fields))
                    if len(matching_input_port_columns) == 0:
                        column_found_in_port_schema = False
                    else:
                        column_found_in_port_schema = True
                        try:
                            datatype_diffs = get_spark_datatypes_diff(
                                spark_type1=rule_input.dataType,
                                spark_type2=matching_input_port_columns[0].dataType,
                                ignore_metadata=True,
                                ignore_nullable=True
                            )
                            schema_mismatch_errors = datatype_diffs
                        except Exception as e:
                            schema_mismatch_errors = {}

                    if (not column_found_in_port_schema) and (target_columns is not None):
                        for target_column in target_columns:
                            # in this case whole input column name should match with previous target column
                            if (rule_input.name == target_column):
                                found_in_target_col = True
                    if (not column_found_in_port_schema) and (not found_in_target_col):
                        if target_columns is None:
                            diag = Diagnostic(
                                "",
                                f"Column: {rule_input.name} not found in input port schema",
                                SeverityLevelEnum.Warning
                            )
                            param_errors[rule_input.name] = [diag]
                        else:
                            diag = Diagnostic(
                                "",
                                f"Column: {rule_input.name} neither found in input port schema nor in expressions.",
                                SeverityLevelEnum.Warning
                            )
                            param_errors[rule_input.name] = [diag]
                    elif column_found_in_port_schema and len(schema_mismatch_errors) > 0:
                        error_msg = []
                        for (path, values) in schema_mismatch_errors.items():
                            error_msg.append(f"schema path: {path} input_param_value: {values[0]} column value: {values[1]}")
                        error_msg = ", ".join(error_msg)
                        diag = Diagnostic(
                            "",
                            f"Schema mismatch between input param and input port column ( {error_msg} )",
                            SeverityLevelEnum.Warning
                        )
                        param_errors[rule_input.name] = [diag]
                    else:
                        param_errors[rule_input.name] = []
                else:
                    param_errors[rule_input.name] = []
            except Exception as e:
                pass
        return param_errors
