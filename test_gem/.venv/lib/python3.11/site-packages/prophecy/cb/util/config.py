from abc import ABC, abstractmethod
from typing import List

from pyspark.sql.types import *


class DataElement(ABC):

    @abstractmethod
    def get_type(self):
        pass

    def to_json(self):
        props = dict()
        props["type"] = self.get_type()
        props["value"] = self.get_value()
        return props

    @abstractmethod
    def get_value(self):
        pass

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        return []


class ConfigurationRecordField():
    def __init__(self, name: str, kind: DataElement, optional: bool, **kwargs):
        self.name = name
        self.kind = kind
        self.optional = optional

    def get_value(self):
        return {self.name: self.kind.get_value()}

    def to_json(self):
        props = dict()
        props["name"] = self.name
        props["kind"] = self.kind.to_json()
        props["optional"] = self.optional
        return props

    def validate_with_spark_schema(self, config_path: list, struct_field: StructField, name_check: bool=True):
        errors = []
        if name_check:
            if (self.name != struct_field.name):
                errors.append((config_path,
                    f"Name mismatch between config field and struct field. ConfigPath: {'.'.join(config_path)} ConfigField Name: {self.name} StructField Name:{struct_field.name}"))
            else:
                pass
        else:
            errors = self.kind.validate_with_spark_schema(config_path, struct_field.dataType)
        return errors


class ConfigurationRecord(DataElement):
    def __init__(self, fields: List[ConfigurationRecordField], **kwargs):
        self.fields = fields

    def get_fields(self):
        return self.fields

    def get_type(self):
        return "record"

    def get_value(self):
        values = dict()
        for field in self.fields:
            values = {**values, **field.get_value()}
        return values

    def to_json(self):
        props = dict()
        props["type"] = self.get_type()
        props["fields"] = [field.to_json() for field in self.get_fields()]
        return props

    def get_config_field_by_name(self, name: str) -> ConfigurationRecordField:
        for field in self.fields:
            if field.name == name:
                return field

    def validate_with_spark_schema(self, config_path: list, dataType: StructType):
        errors = []
        idx = 0
        if isinstance(dataType, StructType):
            for field in self.fields:
                idx = idx + 1
                matching_columns: List[StructField] = list(filter(lambda x: x.name == field.name, dataType.fields))
                if len(matching_columns) > 0:
                    struct_field: StructField = matching_columns[0]
                    errors = errors + field.kind.validate_with_spark_schema(config_path + [f"{field.name}"],
                                                                            struct_field.dataType)
                else:
                    errors.append((config_path + [field.name], f"Missing field. Name: {field.name}"))
        else:
            errors.append((config_path, f"Datatype mismatch. ConfigType:: record ColumnType:: {dataType.typeName()}"))
        return errors


class IntElement(DataElement):

    def __init__(self, value: int, **kwargs):
        self.value = value

    def get_type(self):
        return "int"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [ByteType.typeName(), IntegerType.typeName(), LongType.typeName(),
                                    ShortType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class StringElement(DataElement):
    def __init__(self, value: str, **kwargs):
        self.value = value

    def get_type(self):
        return "str"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [StringType.typeName(), DateType.typeName(), TimestampType.typeName(), ByteType.typeName(), BinaryType.typeName(), DecimalType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class BooleanElement(DataElement):
    def __init__(self, value: bool, **kwargs):
        self.value = value

    def get_type(self):
        return "bool"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [BooleanType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class FloatElement(DataElement):
    def __init__(self, value: float, **kwargs):
        self.value = value

    def get_type(self):
        return "float"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [DecimalType.typeName(), DoubleType.typeName(), FloatType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class LongElement(DataElement):
    def __init__(self, value: int, **kwargs):
        self.value = value

    def get_type(self):
        return "long"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [ByteType.typeName(), IntegerType.typeName(), LongType.typeName(),
                                    ShortType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class DoubleElement(DataElement):
    def __init__(self, value: float, **kwargs):
        self.value = value

    def get_type(self):
        return "double"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [DecimalType.typeName(), DoubleType.typeName(), FloatType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class ShortElement(DataElement):
    def __init__(self, value: int, **kwargs):
        self.value = value

    def get_type(self):
        return "short"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [ByteType.typeName(), IntegerType.typeName(), LongType.typeName(),
                                    ShortType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class SparkExpressionElement(DataElement):
    def __init__(self, value: str, **kwargs):
        self.value = value

    def get_type(self):
        return "spark_expression"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [StringType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class ArrayElement(DataElement):
    def __init__(self, elementType: DataElement, value: list, **kwargs):
        self.elementType = elementType
        self.value = value

    def get_type(self):
        return "array"

    def get_value(self):
        return self.value

    def get_elementType(self):
        return self.elementType

    def to_json(self):
        props = super().to_json()
        props["elementType"] = self.get_elementType().to_json()
        return props

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (isinstance(dataType, ArrayType)):
            errors = self.elementType.validate_with_spark_schema(config_path + ["elementType"], dataType.elementType)
            return errors
        else:
            return [(config_path,
                     f"Datatype mismatch. Config datatype: {self.get_type()} Spark datatype: {dataType.typeName()}")]


class DBSecretElement(DataElement):
    def __init__(self, value: str, **kwargs):
        self.value = value

    def get_type(self):
        return "databricks_secret"

    def get_value(self):
        return self.value

    def get_key(self):
        return self.value.split(":")[0]

    def get_value(self):
        return self.value.split(":")[1]

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if (dataType.typeName() in [StringType.typeName()]):
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]


class SecretElement(DataElement):
    def __init__(self, fields: List[ConfigurationRecordField], **kwargs):
        self.fields = fields

    def get_fields(self):
        return self.fields

    def get_type(self):
        return "secret"

    def get_value(self):
        values = dict()
        for field in self.fields:
            values = {**values, **field.get_value()}
        return values

    def to_json(self):
        props = dict()
        props["type"] = self.get_type()
        props["fields"] = [field.to_json() for field in self.get_fields()]
        return props

    def get_config_field_by_name(self, name: str) -> ConfigurationRecordField:
        for field in self.fields:
            if field.name == name:
                return field

    def validate_with_spark_schema(self, config_path: list, dataType: StructType):
        errors = []
        idx = 0
        if isinstance(dataType, StructType):
            for field in self.fields:
                idx = idx + 1
                matching_columns: List[StructField] = list(filter(lambda x: x.name == field.name, dataType.fields))
                if len(matching_columns) > 0:
                    struct_field: StructField = matching_columns[0]
                    errors = errors + field.kind.validate_with_spark_schema(config_path + [f"{field.name}"],
                                                                            struct_field.dataType)
                else:
                    errors.append((config_path + [field.name], f"Missing field. Name: {field.name}"))
        else:
            errors.append((config_path, f"Datatype mismatch. ConfigType:: record ColumnType:: {dataType.typeName()}"))
        return errors

class DateElement(DataElement):
    def __init__(self, value: str, **kwargs):
        self.value = value

    def get_type(self):
        return "date"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if dataType.typeName() in [DateType.typeName()]:
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]

class TimestampElement(DataElement):
    def __init__(self, value: str, **kwargs):
        self.value = value

    def get_type(self):
        return "date"

    def get_value(self):
        return self.value

    def validate_with_spark_schema(self, config_path: list, dataType: DataType):
        if dataType.typeName() in [TimestampType.typeName(), TimestampNTZType.typeName()]:
            return []
        else:
            return [(config_path,
                     f"DataType mismatch. Config datatype:{self.get_type()} Spark datatype: {dataType.typeName()}")]

class Configuration():
    def __init__(self, record: ConfigurationRecord):
        self.record = record
        pass

    def get_record(self) -> ConfigurationRecord:
        return self.record

    def get_value(self):
        return self.record.get_value()

    def get_config_field_names(self):
        names = []
        for field in self.record.get_fields():
            names.append(field.name)
        return names

    def validate_with_spark_schema(self, dataType: DataType):
        diags = []
        if isinstance(dataType, StructType):
            errors = self.record.validate_with_spark_schema([], dataType)
            for error in errors:
                path, errmsg = error
                diags.append(f"Mismatch at field: {'.'.join(path)}. Error: {errmsg}")
        else:
            diags.append(f"incorrect datatype. Expected: StructType Found: {dataType}")
        return diags
