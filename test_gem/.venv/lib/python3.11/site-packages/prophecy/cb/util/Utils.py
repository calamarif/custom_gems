from typing import Dict, Tuple

from pyspark.sql import SparkSession
from pyspark.sql.types import *
import random, string
from datetime import datetime, date
from decimal import Decimal


def createScalaList(l, spark):
    return spark.sparkContext._jvm.PythonUtils.toList(l)
    # res = spark.sparkContext._jvm.scala.collection.immutable.Nil
    # for x in reversed(l):
    #     res = getattr(res, "$colon$colon")(x)
    # return res


def add_dummy_row_to_dataframe(spark: SparkSession, dataframe):
    def dummy_value_for_spark_data_type(data_type):
        if isinstance(data_type, StringType):
            return "DummyString"
        elif isinstance(data_type, IntegerType):
            return 0
        elif isinstance(data_type, LongType):
            return 0
        elif isinstance(data_type, DoubleType):
            return 0.0
        elif isinstance(data_type, FloatType):
            return 0.0
        elif isinstance(data_type, BooleanType):
            return True
        elif isinstance(data_type, ByteType):
            return 0
        elif isinstance(data_type, ShortType):
            return 0
        elif isinstance(data_type, BinaryType):
            return bytearray()
        elif isinstance(data_type, TimestampType):
            return datetime(2022, 1, 1, 12, 30, 0)
        elif isinstance(data_type, DateType):
            return date(2022, 1, 1)
        elif isinstance(data_type, DecimalType):
            return Decimal('0.0')
        elif isinstance(data_type, StructType):
            dummy_values = []
            for field in data_type.fields:
                dummy_values.append(dummy_value_for_spark_data_type(field.dataType))
            return Row(*dummy_values)
        elif isinstance(data_type, ArrayType):
            dummy_values = ()
            element_type = data_type.elementType
            dummy_values = dummy_values + (dummy_value_for_spark_data_type(element_type),)
            return dummy_values
        elif isinstance(data_type, MapType):
            dummy_key = dummy_value_for_spark_data_type(data_type.keyType)
            dummy_value = dummy_value_for_spark_data_type(data_type.valueType)
            return {dummy_key: dummy_value}
        elif isinstance(data_type, NullType):
            return None

    new_df = spark.createDataFrame([dummy_value_for_spark_data_type(dataframe.schema)], dataframe.schema)
    return dataframe.union(new_df)


def generate_element_id():
    """
    This will generate an unique id for all the workflow.json elements. Like, NodePorts, WorkflowEdges, WorkflowNodes etc.
    """

    # Generate a random alphanumeric string of length 21
    def generate_random_string():
        alphanumeric_chars = string.ascii_letters + string.digits
        return ''.join(random.choice(alphanumeric_chars) for _ in range(21))

    # Generate two random strings
    random_string1 = generate_random_string()
    random_string2 = generate_random_string()

    # Concatenate the strings with '$$' in between
    concatenated_strings = random_string1 + '$$' + random_string2

    return concatenated_strings


def get_spark_datatypes_diff(spark_type1: DataType, spark_type2: DataType, ignore_metadata=False, ignore_nullable=False):
    """
    Returns a dictionary with key as diffPath and value as (spark_type1_value, spark_type2_value).
    Empty dictionary in case there is no diff found.
    ignore_metadata -> bool : Ignore the metadata field diff
    ignore_nullable -> bool : Ignore the nullable field diff
    """
    import copy
    def clear_metadata_nullable(datatype: DataType):
        if (isinstance(datatype, StructType)):
            for field in datatype.fields:
                # Convert all the column names to lower case so that it doesn't show a diff
                # for same column name but different case.
                field.name = field.name.lower()
                if ignore_metadata:
                    field.metadata = None
                if ignore_nullable:
                    field.nullable = True
                clear_metadata_nullable(field.dataType)
        elif (isinstance(datatype, ArrayType)):
            clear_metadata_nullable(datatype.elementType)
            if ignore_nullable:
                datatype.containsNull = True
        elif (isinstance(datatype, MapType)):
            clear_metadata_nullable(datatype.keyType)
            clear_metadata_nullable(datatype.valueType)
            if ignore_nullable:
                datatype.valueContainsNull = True
        else:
            pass

    # create a separate copy so that it doesn't modify the original schema datatype fields.
    spark_type1 = copy.deepcopy(spark_type1)
    spark_type2 = copy.deepcopy(spark_type2)
    clear_metadata_nullable(spark_type1)
    clear_metadata_nullable(spark_type2)
    type1_json = spark_type1.jsonValue()
    type2_json = spark_type2.jsonValue()
    return get_json_diff(type1_json, type2_json)

def get_json_diff(dict1, dict2, path='') -> Dict[str, Tuple]:
    """
    Return granular diff paths along with diffs for two python dictionaries.
    key: "a.b.c[0].d"  value: ("dict1_value", "dict2_value")
    """
    def get_element_diff(element1, element2, initial_path: str):
        element_diffs = {}
        if isinstance(element1, dict) and isinstance(element2, dict):
            for key in element1.keys() | element2.keys():
                new_path = f"{initial_path}.{key}" if initial_path else f"{key}"
                if (key not in element1) and (key in element2):
                    element_diffs[new_path] = (element1.get(key), element2.get(key))
                elif (key in element1) and (key not in element2):
                    element_diffs[new_path] = (element1.get(key), element2.get(key))
                elif (key in element1) and (key in element2):
                    element_diffs = {**element_diffs,
                                     **get_element_diff(element1.get(key), element2.get(key), new_path)}
                else:
                    pass
        elif isinstance(element1, list) and isinstance(element2, list):
            if len(element1) != len(element2):
                element_diffs[initial_path] = (element1, element2)
            else:
                for index in range(0, len(element1)):
                    new_path = f"{initial_path}[{index}]"
                    element_diffs = {**element_diffs, **get_element_diff(element1[index], element2[index], new_path)}
        elif type(element1) == type(element2):
            if element1 != element2:
                element_diffs[initial_path] = (element1, element2)
        else:
            element_diffs[initial_path] = (element1, element2)
        return element_diffs

    return get_element_diff(dict1, dict2, path)
