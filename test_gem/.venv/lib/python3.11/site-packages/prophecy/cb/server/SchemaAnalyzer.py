import json

from py4j.protocol import Py4JJavaError
from pyspark import SparkContext
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import StructType
from typing import List, Dict

from prophecy.cb.server.base.ComponentBuilderBase import Diagnostic, ComponentCode, NodePort, SeverityLevelEnum
from prophecy.cb.server.LineageAnalyzer import *
from prophecy.cb.util.Utils import createScalaList, add_dummy_row_to_dataframe

class SchemaAnalysisDummyException(Exception):
    def __init__(self, *elements):
        self.elements = elements

class DataFrameMetadata:
    def __init__(self, schema: StructType = None, isStreaming: bool = False):
        self.schema: StructType = schema
        self.isStreaming: bool = isStreaming


class SchemaAnalyzerResult:
    def __init__(self):
        self.dataFrameMetadata: Dict[str, DataFrameMetadata] = {}
        self.dependencies: str = ""
        self.diagnostics: List[Diagnostic] = []
        self.lineage = ''


def emptyDF(spark: SparkSession, sc: SparkContext, schema: StructType = None, isStreaming: bool = False) -> DataFrame:
    if isStreaming:
        return spark.readStream.schema(schema).csv("/dev/null")
    else:
        return spark.createDataFrame(sc.emptyRDD(), schema)

def df_with_dummy_row(spark: SparkSession, sc: SparkContext, schema: StructType = None, isStreaming: bool = False) -> DataFrame:
    if isStreaming:
        return spark.readStream.schema(schema).csv("/dev/null")
    else:
        return add_dummy_row_to_dataframe(spark, spark.createDataFrame(sc.emptyRDD(), schema))

def AnalyzeSchema(spark: SparkSession,
                  sc: SparkContext,
                  process_id: str,
                  process_name: str,
                  component: str,
                  inputs: List[NodePort],
                  outputs: List[NodePort],
                  lineageEnabled: bool,
                  applyFunc,
                  add_dummy_rows: bool = False
                  ) -> SchemaAnalyzerResult:
    schemaAnalyzerResult = SchemaAnalyzerResult()
    inputDataFrames = []
    outputDataFrames = None

    if len(inputs) > 0:
        for input in inputs:
            if add_dummy_rows:
               inputDataFrames.append(df_with_dummy_row(spark, sc, input.schema, input.isStreaming))
            else:
                inputDataFrames.append(emptyDF(spark, sc, input.schema, input.isStreaming))
        try:
            outputDataFrames = applyFunc(spark, *inputDataFrames)
        except Py4JJavaError as e:
            schemaAnalyzerResult.diagnostics.append(Diagnostic(process_id, str(e), SeverityLevelEnum.Error))
    else:
        try:
            outputDataFrames = applyFunc(spark)
        except Py4JJavaError as e:
            schemaAnalyzerResult.diagnostics.append(Diagnostic(process_id, str(e), SeverityLevelEnum.Error))
    if outputDataFrames is not None:
        outputDataFrames = list(outputDataFrames) if type(outputDataFrames) is tuple else [outputDataFrames]
    else:
        outputDataFrames = []
    if (lineageEnabled):
        schemaAnalyzerResult.lineage = analyzeLineage(process_id, process_name, component, [x.id for x in inputs],
                                                      [x.id for x in outputs], inputDataFrames, outputDataFrames,
                                                      spark)
    if len(outputDataFrames) != len(outputs):
        schemaAnalyzerResult.diagnostics.append(Diagnostic(process_id,
                                                           f"Schema Analysis found {len(outputDataFrames)} schema's but output ports defined were {len(outputs)}.",
                                                           SeverityLevelEnum.Error))
        return schemaAnalyzerResult
    else:
        for idx, dataFrame in enumerate(outputDataFrames):
            schemaAnalyzerResult.dataFrameMetadata[outputs[idx].id] = DataFrameMetadata(dataFrame.schema,
                                                                                        dataFrame.isStreaming)

    inputDataframes = dict(zip(map(lambda x: x.id, inputs), inputDataFrames))
    outputDataframes = dict(zip(map(lambda x: x.id, outputs), outputDataFrames))

    cdc = spark.sparkContext._jvm.io.prophecy.libs.CDC()
    dependencies = cdc.calculateDependenciesJson(
        spark._jsparkSession,
        createScalaList([x._jdf for x in outputDataframes.values()], spark),
        createScalaList([x for x in outputDataframes.keys()], spark),
        createScalaList([x._jdf for x in inputDataframes.values()], spark),
        createScalaList([x for x in inputDataframes.keys()], spark)
    )
    schemaAnalyzerResult.dependencies = json.loads(dependencies)

    return schemaAnalyzerResult
