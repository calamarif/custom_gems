from dataclasses import dataclass
from typing import List, Optional

from prophecy.cb.server.airflow.common import SettingsTab
from prophecy.cb.server.base.ComponentBuilderBase import (
    Component,
    ComponentCode,
    ComponentProperties,
    ComponentSpec,
    Diagnostic,
    SeverityLevelEnum,
)
from prophecy.cb.server.base.WorkflowContext import (
    WorkflowContext,
)
from prophecy.cb.ui.uispec import *
from prophecy.cb.util.StringUtils import isBlank
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.providers.databricks.hooks.databricks import DatabricksHook
from databricks import sql
from airflow.hooks.base import BaseHook
import pandas as pd
import pantab
import tableauserverclient as TSC
import yaml

class TableauExtractSpec(ComponentSpec):
    name: str = "TableauExtract"
    category: str = "Data Transfer"
    gemDescription: str = """This operator Generate Tableau Hyper files from Snowflake."""

    @staticmethod
    def properties_tab():
        return (
            StackLayout(gap="1rem", width="40%", direction="vertical")
            .addElement(TitleElement("Source Configuration"))
            .addElement(
                SelectBox("*Source Type", helpText="Data warehouse  for the Table")
                .addOption("Snowflake", "SNOWFLAKE")
                .addOption("Databricks SQL", "DATABRICKS")
                .bindProperty("source_type")
            )
            .addElement(
                Condition()
                .ifEqual(
                    PropExpr("component.properties.source_type"),
                    StringExpr("SNOWFLAKE"),
                )
                .then(
                    SelectBox(
                        titleVar="*Snowflake Connection name",
                        placeholder="snowflake_default",
                        helpText="The connection added in Prophecy for Snowflake auth"
                    )
                    .withSearchEnabled()
                    .withNoContentMessage(
                        "No more Snowflake Connections found in current fabric"
                    )
                    .bindProperty("warehouse_conn_id")
                    .bindOptionProperty("${$.metadata.connections.snowflakedirect}")
                )
                .otherwise(
                    SelectBox(
                        titleVar="*Databricks Connection name",
                        placeholder="databricks_default",
                        helpText="The connection added in Prophecy for Databricks auth"
                    )
                    .withSearchEnabled()
                    .withNoContentMessage(
                        "No more Databricks Connections found in current fabric"
                    )
                    .bindProperty("warehouse_conn_id")
                    .bindOptionProperty("${$.metadata.connections.databrickssql}")
                )
            )
            .addElement(
                Condition()
                .ifEqual(
                    PropExpr("component.properties.source_type"),
                    StringExpr("DATABRICKS"),
                )
                .then(
                    Checkbox("Use Unity Catalog").withHint("Use Unity catalog table")
                    .bindProperty("use_catalog")
                )
            )
            .addElement(
                Condition()
                .ifEqual(
                    PropExpr("component.properties.use_catalog"),
                    BooleanExpr(True),
                )
                .then(
                    TextBox("*Catalog Name", placeholder="hive_metastore",
                            helpText="Catalog name in SQL Warehouse").bindProperty(
                        "catalog_name"
                    )
                )
            )
            .addElement(
                Condition()
                .ifEqual(
                    PropExpr("component.properties.source_type"),
                    StringExpr("DATABRICKS"),
                )
                .then(
                    TextBox("*Database Name", placeholder="my_database",
                            helpText="Source Database in Databricks").bindProperty(
                        "database_name"
                    )
                )
            )
            .addElement(
                TextBox("*Source Table", placeholder="customers",
                        helpText="Source Table which need to be exported").bindProperty(
                    "table_name"
                )
            )
            .addElement(TitleElement("Target Tableau Configuration"))
            .addElement(
                SelectBox(
                    titleVar="*Tableau Connection name",
                    placeholder="tableau_default",
                    helpText="The connection added in Prophecy for Tableau auth"
                )
                .withSearchEnabled()
                .withNoContentMessage(
                    "No more Tableau Connections found in current fabric"
                )
                .bindProperty("tableau_conn_id")
                .bindOptionProperty("${$.metadata.connections.tableau}")

            )
            .addElement(
                TextBox("*Tableau Project Name", placeholder="Samples",
                        helpText="This is the Tableau project where the extract is uploaded"
                        ).bindProperty(
                    "tableau_project_name"
                )
            )
            .addElement(
                TextBox("*Tableau Extract Name", placeholder="customers",
                        helpText="Designated Tableau upload path"
                        ).bindProperty(
                    "tableau_extract_name"
                )
            )
        )

    def dialog(self) -> Dialog:
        return Dialog("Tableau Extract Operator").addElement(
            StackLayout(direction="horizontal").addElement(
                FieldPickerWithTabs("Properties")
                .addTab(
                    PickerTab("Gem Settings", "GemSettings", False, propertyVar="component.settings")
                    .addFields(SettingsTab.settings_field_picker().fields)
                )
            )
            .addElement(TableauExtractSpec.properties_tab())
        )

    def optimizeCode(self) -> bool:
        return True  # TODO: Is it possible to make it True without messing it up

    @dataclass(frozen=True)
    class TableauExtractProperties(ComponentProperties):
        taskId: Optional[str] = None
        source_type: str = "SNOWFLAKE"
        warehouse_conn_id: Optional[str] = None
        databricks_conn_id: Optional[str] = None
        table_name: Optional[str] = None
        database_name: Optional[str] = None
        catalog_name: Optional[str] = None
        use_catalog: bool = False
        dbt_profile: Optional[str] = None
        profiles_dir: Optional[str] = None
        tableau_conn_id: Optional[str] = None
        tableau_project_name: Optional[str] = None
        tableau_extract_name: Optional[str] = None

    def validate(
            self, context: WorkflowContext, component: Component[TableauExtractProperties]
    ) -> List[Diagnostic]:
        diagnostics = []
        props = component.properties
        if isBlank(props.warehouse_conn_id):
            diagnostics.append(
                Diagnostic(
                    "properties.warehouse_conn_id",
                    "Choose an airflow connection for Source(Snowflake/Databricks)",
                    SeverityLevelEnum.Error,
                )
            )
        if isBlank(props.tableau_conn_id):
            diagnostics.append(
                Diagnostic(
                    "properties.tableau_conn_id",
                    "Choose an airflow connection for Target(Tableau)",
                    SeverityLevelEnum.Error,
                )
            )
        if isBlank(props.table_name):
            diagnostics.append(
                Diagnostic(
                    "properties.table_name",
                    "Please provide Table name",
                    SeverityLevelEnum.Error,
                )
            )
        if isBlank(props.tableau_project_name):
            diagnostics.append(
                Diagnostic(
                    "properties.tableau_project_name",
                    "Please provide Tableau Project name",
                    SeverityLevelEnum.Error,
                )
            )
        if isBlank(props.tableau_extract_name):
            diagnostics.append(
                Diagnostic(
                    "properties.tableau_extract_name",
                    "Please provide Tableau extract name",
                    SeverityLevelEnum.Error,
                )
            )

        return diagnostics

    def onChange(
            self,
            context: WorkflowContext,
            oldState: Component[TableauExtractProperties],
            newState: Component[TableauExtractProperties],
    ) -> Component[TableauExtractProperties]:
        return newState

    class TableauExtractCode(ComponentCode):
        def __init__(self, newProps, newSettings):
            self.props: TableauExtractSpec.TableauExtractProperties = newProps
            self.settings: dict = newSettings

        @staticmethod
        def get_table_name(catalog, database, table):
            if database and catalog:
                return f"{catalog}.{database}.{table}"
            elif database:
                return f"{database}.{table}"
            else:
                return table

        @staticmethod
        def get_info(_source_type, warehouse_conn_id, table_name, database_name, catalog_name):
            table = table_name if _source_type == "SNOWFLAKE" else get_table_name(catalog_name, database_name,
                                                                                  table_name)
            hook = SnowflakeHook(warehouse_conn_id) if _source_type == "SNOWFLAKE" else DatabricksHook(
                warehouse_conn_id)
            return hook, table

        @staticmethod
        def get_databricks_sql_conn(profile_dir, dbt_profile):
            def read_yaml_content():
                file_path = f"{profile_dir}/profiles.yml"
                with open(file_path, 'r') as file:
                    data = yaml.safe_load(file)
                    target = data[dbt_profile]['target']
                    db_creds = data[dbt_profile]['outputs'][target]
                    return (db_creds['host'], db_creds['http_path'], db_creds['token'])

            db_creds = read_yaml_content()
            return sql.connect(
                server_hostname=db_creds[0],
                http_path=db_creds[1],
                access_token=db_creds[2]
            )

        @staticmethod
        def export_tableau_hyperfile(source_type, hyper_path, hyper_name, tableau_conn_id, project_name,
                                     warehouse_conn_id, table_name, database_name, catalog_name, profile_dir,
                                     dbt_profile):

            hook, table = get_info(source_type, warehouse_conn_id, table_name, database_name, catalog_name)
            sql_query = f"SELECT * FROM {table}"
            # Fetch data from Snowflake into a pandas DataFrame
            # tableau
            connection = hook.get_conn() if source_type == "SNOWFLAKE" else get_databricks_sql_conn(profile_dir,
                                                                                                    dbt_profile)
            cursor = connection.cursor()
            try:
                # Execute the query
                cursor.execute(sql_query)
                results = cursor.fetchall()
                if results:
                    df = pd.DataFrame(results, columns=[col[0] for col in cursor.description])
                    print(f"Data fetched successfully from {source_type}.")

                    # Specify the path for the Hyper file
                    pantab.frame_to_hyper(df, hyper_path, table=hyper_name)
                    print(f"Data written to Hyper file successfully at {hyper_path}.")
                else:
                    print("Query returned no data.")
            except Exception as e:
                print(f"An error occurred while fetching data: {e}")
                raise

            # Get Tableau details from Airflow connection
            tableau_conn = BaseHook.get_connection(tableau_conn_id)

            # Tableau authentication ( using Username/Password )
            # tableau_auth = TSC.TableauAuth(tableau_conn.login, tableau_conn.password,
            #                                site_id=tableau_conn.extra_dejson.get('site_id'))

            # Using Personal Access Token
            tableau_auth = TSC.PersonalAccessTokenAuth(tableau_conn.extra_dejson.get('token_name'),
                                                       tableau_conn.extra_dejson.get('personal_access_token'),
                                                       site_id=tableau_conn.extra_dejson.get('site_id'))

            server = TSC.Server(tableau_conn.host, use_server_version=True)

            with server.auth.sign_in(tableau_auth):
                all_projects = TSC.Pager(server.projects.get)
                project = [project for project in all_projects if project.name == project_name][0]
                print("Writing into project: " + str(project.name))
                # Publish Hyper file to Tableau Server
                new_datasource_item = TSC.DatasourceItem(project.id)
                datasource = server.datasources.publish(new_datasource_item, hyper_path, 'Overwrite')
                print("Datasource published. ID: ", datasource.id)

        def apply(self):
            from airflow.operators.python import PythonOperator
            from datetime import timedelta

            tableau_conn_id = self.props.tableau_conn_id
            project_name = self.props.tableau_project_name
            hyper_path = f"{self.props.tableau_extract_name}.hyper"
            hyper_name = 'Extract'  # this can be handled internally
            source_type = self.props.source_type
            # snowflake
            warehouse_conn_id = self.props.warehouse_conn_id
            table_name = self.props.table_name
            database_name = self.props.database_name
            catalog_name = self.props.catalog_name

            return PythonOperator(
                task_id=self.props.taskId,
                python_callable=export_tableau_hyperfile,
                op_kwargs={"source_type": source_type, "hyper_path": hyper_path, "hyper_name": hyper_name,
                           "tableau_conn_id": tableau_conn_id, "project_name": project_name,
                           "warehouse_conn_id": warehouse_conn_id, "table_name": table_name,
                           "database_name": database_name, "catalog_name": catalog_name,
                           "profile_dir": self.props.profiles_dir, "dbt_profile": self.props.dbt_profile},
                **self.settings

            )
